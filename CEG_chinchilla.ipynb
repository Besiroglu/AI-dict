{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Besiroglu/AI-dict/blob/master/CEG_chinchilla.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "LtBNOy3IwKQf"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy.optimize import basinhopping\n",
        "import pandas as pd\n",
        "\n",
        "#Kaplan\n",
        "# Define the function L(N, D) as per the provided mathematical formula\n",
        "def L_kaplan_with_compute(D, compute, alpha_N=0.076, alpha_D=0.103, N_c=6.4e13, D_c=1.8e13):\n",
        "    # Calculate the function value\n",
        "    N = compute/(6*D)\n",
        "    value = ((N_c / N)**(alpha_N / alpha_D) + (D_c / D))**alpha_D\n",
        "    return value\n",
        "\n",
        "# Define the new function L(N, D) as per the provided mathematical formula\n",
        "def L_chinchilla_with_compute(D, compute, E=1.69, A=406.4, B=410.7):\n",
        "    N = compute / (6 * D)\n",
        "    # Calculate the function value\n",
        "    value = E + A / N**0.34 + B / D**0.28\n",
        "    return value\n",
        "\n",
        "# Define the new function L(N, D) as per the provided mathematical formula\n",
        "def L_model_term(D, compute, E=1.69, A=406.4, B=410.7):\n",
        "    N = compute / (6 * D)\n",
        "    # Calculate the function value\n",
        "    value = A / N**0.34\n",
        "    return value\n",
        "\n",
        "# Define the new function L(N, D) as per the provided mathematical formula\n",
        "def L_data_term(D, compute, E=1.69, A=406.4, B=410.7):\n",
        "    N = compute / (6 * D)\n",
        "    # Calculate the function value\n",
        "    value = B / D**0.28\n",
        "    return value\n",
        "\n",
        "# A function to perform the basinhopping optimization for L_chinchilla\n",
        "def optimize_L_chinchilla(compute, initial_guess_D):\n",
        "    result = basinhopping(\n",
        "        L_chinchilla_with_compute,  # The function to minimize\n",
        "        initial_guess_D,  # Initial guess for the variables\n",
        "        minimizer_kwargs={\n",
        "            'method': 'L-BFGS-B',\n",
        "            'bounds': [(initial_guess_D/10000, initial_guess_D*10000)],\n",
        "            'args': (compute,),  # Additional arguments passed to the objective function\n",
        "            'options': {'ftol': tolerance},\n",
        "        },\n",
        "        niter=max_iterations,\n",
        "        stepsize=step_size\n",
        "    )\n",
        "    return result.x, result.fun"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qigJETtbXNz8",
        "outputId": "e5b3277d-a873-411f-a909-09c57946f61e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "compute budget 1e+20\n",
            "optimal data Kaplan [1.01202056e+10]\n",
            "loss Kaplan-scale [2.63747885]\n",
            "loss Chinchilla-scale [2.59984975]\n"
          ]
        }
      ],
      "source": [
        "import math\n",
        "import random\n",
        "\n",
        "# DataFrame to store the results\n",
        "# Initialize DataFrame to store the results\n",
        "results_df = pd.DataFrame(columns=[\n",
        "    'Compute', 'CEG', 'Optimal D (Chinchilla)', 'Optimal D (Kaplan)',\n",
        "    'Chinchilla Loss', 'Compute_needed Chinchilla to match Kaplan',\n",
        "    'Loss model term (On Kaplan-scaling)', 'Loss data term (On Kaplan-scaling)'\n",
        "])\n",
        "\n",
        "compute_values = np.geomspace(1e20, 1e30, num=int(np.log2(1e30/1e20))+1)\n",
        "\n",
        "compute = compute_values[0]\n",
        "initial_guess_D = 10*compute**0.5\n",
        "\n",
        "step_size = 0.0005\n",
        "max_iterations = 10000\n",
        "\n",
        "tolerance = 1e-4 # Tolerance for the difference in loss\n",
        "\n",
        "for compute in compute_values:\n",
        "  print(\"compute budget\", compute)\n",
        "\n",
        "  # Use the basinhopping algorithm to minimize L_kaplan with the given bounds\n",
        "  result_basinhopping_kaplan = basinhopping(\n",
        "      L_kaplan_with_compute,  # This function should be defined to accept 'compute' as an argument\n",
        "      initial_guess_D,\n",
        "      minimizer_kwargs={\n",
        "          'method': 'L-BFGS-B',\n",
        "          'bounds': [(compute**0.2, compute**0.85)],\n",
        "          'args': (compute,),  # Pass 'compute' as an additional argument\n",
        "          'options': {'ftol': tolerance/5}\n",
        "      },\n",
        "      niter=max_iterations,\n",
        "      stepsize=initial_guess_D*step_size\n",
        "  )\n",
        "\n",
        "  # Use the basinhopping algorithm to minimize L_kaplan with the given bounds\n",
        "  result_basinhopping_chinchilla = basinhopping(\n",
        "      L_chinchilla_with_compute,  # This function should be defined to accept 'compute' as an argument\n",
        "      initial_guess_D,\n",
        "      minimizer_kwargs={\n",
        "          'method': 'L-BFGS-B',\n",
        "          'bounds': [(compute**0.25, compute**0.85)],\n",
        "          'args': (compute,),  # Pass 'compute' as an additional argument\n",
        "          'options': {'ftol': tolerance/5}\n",
        "      },\n",
        "      niter=max_iterations,\n",
        "      stepsize=initial_guess_D*step_size\n",
        "  )\n",
        "\n",
        "  loss_kaplan = L_chinchilla_with_compute(result_basinhopping_kaplan.x, compute)\n",
        "  loss_chinchilla_1 = L_chinchilla_with_compute(result_basinhopping_chinchilla.x, compute)\n",
        "\n",
        "  print(\"optimal data Kaplan\", result_basinhopping_kaplan.x)\n",
        "\n",
        "  print(\"loss Kaplan-scale\", loss_kaplan)\n",
        "  print(\"loss Chinchilla-scale\", loss_chinchilla_1)\n",
        "\n",
        "  compute_needed = compute\n",
        "\n",
        "  max_iterations = max_iterations\n",
        "  for iteration in range(max_iterations):\n",
        "      initial_guess_D = compute_needed**0.5\n",
        "\n",
        "      # Use the basinhopping algorithm to minimize L_kaplan with the given bounds\n",
        "      result_basinhopping_chinchilla = basinhopping(\n",
        "          L_chinchilla_with_compute,  # This function should be defined to accept 'compute' as an argument\n",
        "          initial_guess_D,\n",
        "          minimizer_kwargs={\n",
        "              'method': 'L-BFGS-B',\n",
        "              'bounds': [(compute_needed**0.2, compute_needed**0.85)],\n",
        "              'args': (compute_needed,),  # Pass 'compute' as an additional argument\n",
        "              'options': {'ftol': tolerance}\n",
        "          },\n",
        "          niter=max_iterations,\n",
        "          stepsize=initial_guess_D*step_size\n",
        "      )\n",
        "\n",
        "      loss_chinchilla = L_chinchilla_with_compute(result_basinhopping_chinchilla.x, compute)\n",
        "\n",
        "      #print(\"loss Chinchilla:\", loss_chinchilla, \"with compute\", compute_needed)\n",
        "      # Compare the chinchilla loss to the kaplan loss\n",
        "      absolute_difference = abs(loss_chinchilla - loss_kaplan)\n",
        "      if absolute_difference < tolerance*10:\n",
        "          # If the loss is close enough, break the loop\n",
        "          break\n",
        "      elif loss_chinchilla < loss_kaplan:\n",
        "          # If the loss is too low, decrease the compute budget\n",
        "          delta = (1 + 10*(absolute_difference + random.uniform(0, 0.005))+ random.uniform(0, 0.01))\n",
        "\n",
        "          compute_needed /= delta\n",
        "      else:\n",
        "          # If the loss is too high, increase the compute budget\n",
        "          compute_needed *= delta\n",
        "\n",
        "  # Print the final compute budget and the corresponding D\n",
        "  print(f\"Compute_needed: {compute_needed}\")\n",
        "  print(f\"Optimal D: {optimal_D}\")\n",
        "  print(f\"Chinchilla loss: {loss_chinchilla_1}\")\n",
        "  CEG = compute/compute_needed\n",
        "  initial_guess_D = optimal_D\n",
        "\n",
        "  print(\"Loss Kaplan\", L_chinchilla(result_basinhopping_kaplan.x))\n",
        "  print(\"Checking by plugging D_opt back into chinchilla to check it matches Kaplan loss\", L_chinchilla_with_compute(result_basinhopping_chinchilla.x, compute))\n",
        "  print(\"CEG\", CEG)\n",
        "\n",
        "\n",
        "  L_data = L_data_term(result_basinhopping_kaplan.x, compute)\n",
        "  L_model = L_model_term(result_basinhopping_kaplan.x, compute)\n",
        "\n",
        "  # Creating a DataFrame from a dictionary with the current iteration's results\n",
        "  iter_results_df = pd.DataFrame({\n",
        "      'Compute': [compute],\n",
        "      'CEG': [CEG[0]],  # Assuming CEG is a scalar\n",
        "      'Optimal D (Chinchilla)': [result_basinhopping_chinchilla.x[0]],\n",
        "      'Optimal D (Kaplan)': [result_basinhopping_kaplan.x[0]],\n",
        "      'Chinchilla Loss': [loss_chinchilla],\n",
        "      'Compute_needed Chinchilla to match Kaplan': [compute_needed],\n",
        "      'Loss model term (On Kaplan-scaling)': [L_model],\n",
        "      'Loss data term (On Kaplan-scaling)': [L_data]\n",
        "  })\n",
        "\n",
        "  # Append the results of the current iteration to the main results DataFrame\n",
        "  results_df = pd.concat([results_df, iter_results_df], ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results_df_store = results_df.copy()"
      ],
      "metadata": {
        "id": "HjvNHgInevRq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "axkO6rGPszZt"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.optimize import curve_fit\n",
        "\n",
        "# Assuming results_df is already defined and 'linear_fit' function is defined\n",
        "\n",
        "# Convert 'CEG' and 'Compute' columns to numeric values, forcing NaNs if conversion fails\n",
        "results_df['CEG'] = pd.to_numeric(results_df['CEG'])\n",
        "results_df['Compute'] = pd.to_numeric(results_df['Compute'])\n",
        "\n",
        "# Drop rows with NaN values that resulted from conversion failure\n",
        "results_df = results_df.dropna(subset=['CEG', 'Compute'])\n",
        "\n",
        "# Taking the log of 'Compute'\n",
        "log_Compute = np.log(results_df['Compute'])\n",
        "ceg = results_df['CEG']\n",
        "\n",
        "\n",
        "# Plotting the original data and the fitted line\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(results_df['Compute'], results_df['CEG'], label='Data Line', linestyle='-')  # Use linestyle='-' for a solid line\n",
        "plt.xscale('log')  # Applying logarithmic scale to x-axis\n",
        "plt.yscale('log')  # Applying logarithmic scale to x-axis\n",
        "\n",
        "plt.xlabel('Compute')\n",
        "plt.ylabel('Compute equivalent gain')\n",
        "plt.title('Compute Budget Efficiency Gain (CEG) vs Compute Budget')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting Loss model term (On Kaplan-scaling) and Loss data term (On Kaplan-scaling) over Compute in log-log scale\n",
        "plt.figure(figsize=(14, 7))\n",
        "\n",
        "# Loss model term (On Kaplan-scaling) vs Compute\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(results_df['Compute'], results_df['Loss model term (On Kaplan-scaling)'], 'o-', label='Loss model term (On Kaplan-scaling)')\n",
        "plt.xscale('log')\n",
        "plt.yscale('log')\n",
        "plt.xlabel('Compute (FLOP)')\n",
        "plt.ylabel('Loss model term (On Kaplan-scaling)')\n",
        "plt.title('Loss model term (On Kaplan-scaling) vs Compute (FLOP)')\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "\n",
        "# Loss data term (On Kaplan-scaling) vs Compute\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(results_df['Compute'], results_df['Loss data term (On Kaplan-scaling)'], 'o-', label='Loss data term (On Kaplan-scaling)')\n",
        "plt.xscale('log')\n",
        "plt.yscale('log')\n",
        "plt.xlabel('Compute (FLOP)')\n",
        "plt.ylabel('Loss data term (On Kaplan-scaling)')\n",
        "plt.title('Loss data term (On Kaplan-scaling) vs Compute (FLOP)')\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "s_SXm-ApX--M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_df"
      ],
      "metadata": {
        "id": "on96l5DN1DBN"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMxFoyUsKqMUF2Y5MqJoeae",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}